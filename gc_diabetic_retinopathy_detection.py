# -*- coding: utf-8 -*-
"""GC_Diabetic_Retinopathy_Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r5LqD5SRvPs50Q9bYdCjIjr3yrM_sIS7
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install opencv-python

!pip install tensorflow

import cv2
import matplotlib.pyplot as plt
import numpy as np
import os
import pandas as pd
from PIL import Image
import seaborn as sns
import matplotlib.pyplot as plt
import PIL
import plotly
import plotly.express as px
import gc

from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten,Input,ZeroPadding2D,BatchNormalization,Activation
from tensorflow.keras.metrics import BinaryAccuracy, Precision, Recall
#from tensorflow.keras.layers.experimental import preprocessing

from sklearn.metrics import classification_report
import warnings
warnings.filterwarnings("ignore")

path = '/content/drive/MyDrive/Diabetic/gaussian_filtered_images/gaussian_filtered_images'

No_DR_Img = os.listdir(path + '/No_DR/')
Mild_Img = os.listdir(path + '/Mild/')
Moderate_Img = os.listdir(path + '/Moderate/')
Proliferate_DR_Img = os.listdir(path + '/Proliferate_DR/')
Severe_Img = os.listdir(path + '/Severe/')

print("No_DR_Img has {} images".format(len(No_DR_Img)))
print("Mild_Img has {} images".format(len(Mild_Img)))
print("Moderate_Img has {} images".format(len(Moderate_Img)))
print("Proliferate_DR_Img has {} images".format(len(Proliferate_DR_Img)))
print("Severe_Img has {} images".format(len(Severe_Img)))

total_images = len(No_DR_Img) + len(Mild_Img) + len(Moderate_Img) + len(Proliferate_DR_Img) + len(Severe_Img)
print("Total number of images: {}".format(total_images))

#Viewing the dataset
fig = plt.figure(figsize=(16,4))
for i in range(4):
    plt.subplot(1, 4, i+1)
    img = cv2.imread(path+'/No_DR/'+ No_DR_Img[i])
    plt.imshow(img)
plt.suptitle("Retinal Image without Diabetic Retinopathy",fontsize=20)
plt.show()

#Viewing the dataset
fig = plt.figure(figsize=(16,4))
for i in range(4):
    plt.subplot(1, 4, i+1)
    img = cv2.imread(path+'/Mild/'+ Mild_Img[i])
    plt.imshow(img)
plt.suptitle("Retinal Image with Mild Diabetic Retinopathy",fontsize=20)
plt.show()

#Viewing the dataset
fig = plt.figure(figsize=(16,4))
for i in range(4):
    plt.subplot(1, 4, i+1)
    img = cv2.imread(path+'/Moderate/'+ Moderate_Img[i])
    plt.imshow(img)
plt.suptitle("Retinal Image with Moderate Diabetic Retinopathy",fontsize=20)
plt.show()

#Viewing the dataset
fig = plt.figure(figsize=(16,4))
for i in range(4):
    plt.subplot(1, 4, i+1)
    img = cv2.imread(path+'/Severe/'+ Severe_Img[i])
    plt.imshow(img)
plt.suptitle("Retinal Image with Severe Diabetic Retinopathy",fontsize=20)
plt.show()

#Viewing the dataset
fig = plt.figure(figsize=(16,4))
for i in range(4):
    plt.subplot(1, 4, i+1)
    img = cv2.imread(path+'/Proliferate_DR/'+ Proliferate_DR_Img[i])
    plt.imshow(img)
plt.suptitle("Retinal Image with Proliferate Diabetic Retinopathy",fontsize=20)
plt.show()

fig = plt.figure(figsize=(20, 5))

# No_DR
plt.subplot(1, 5, 1)
img_no_dr = cv2.imread(path + '/No_DR/' + No_DR_Img[0])
plt.imshow(img_no_dr)
plt.title('No_DR')
plt.axis('off')

# Mild
plt.subplot(1, 5, 2)
img_mild = cv2.imread(path + '/Mild/' + Mild_Img[0])
plt.imshow(img_mild)
plt.title('Mild')
plt.axis('off')

# Moderate
plt.subplot(1, 5, 3)
img_moderate = cv2.imread(path + '/Moderate/' + Moderate_Img[0])
plt.imshow(img_moderate)
plt.title('Moderate')
plt.axis('off')

# Severe
plt.subplot(1, 5, 4)
img_severe = cv2.imread(path + '/Severe/' + Severe_Img[0])
plt.imshow(img_severe)
plt.title('Severe')
plt.axis('off')

# Proliferate_DR
plt.subplot(1, 5, 5)
img_proliferate = cv2.imread(path + '/Proliferate_DR/' + Proliferate_DR_Img[0])
plt.imshow(img_proliferate)
plt.title('Proliferate_DR')
plt.axis('off')

plt.suptitle("Examples of Diabetic Retinopathy Images", fontsize=20)
plt.tight_layout()
plt.show()

Classes = {'No_DR':0, 'Mild':1, 'Moderate':2, 'Severe':3, 'Proliferate_DR':4}

gc.collect()

#Normalizing pixel values
X = []
y = []
desired_size = (128, 128)

for i in Classes:
    folder_path = path + '/' + i  # Added '/' here
    for j in os.listdir(folder_path):
        img = cv2.imread(folder_path+'/'+j)
        img = cv2.resize(img, desired_size)
        # normalize values
        img = img / 255  #-->Apply normalization because we want pixel values to be scaled to the range 0-1
        X.append(img)
        y.append(Classes[i])

X = np.array(X)
y = np.array(y)

X.shape, y.shape

"""# **Data Augmentation**"""

#Class division

Class_series=pd.Series(y)
lis=["No_DR","Mild","Moderate","Severe","Proliferate_DR"]
DR_or_not = Class_series.value_counts().tolist()
values = [DR_or_not[0], DR_or_not[1], DR_or_not[2], DR_or_not[3], DR_or_not[4]]
fig = px.pie(values=Class_series.value_counts(), names=lis , width=800, height=400, color_discrete_sequence=["skyblue","black","pink","purple","blue"]
             ,title="percentage among the different Severities of DR")
fig.show()

!pip install imbalanced-learn

from imblearn.over_sampling import SMOTE
# Using SMOTE to oversample the minority class(Edema) to avoid class imbalance
sm = SMOTE(random_state = 2)
X, y = sm.fit_resample(X.reshape(X.shape[0], -1), y.ravel())

#Class division

Class_series=pd.Series(y)
lis=["No_DR","Mild","Moderate","Severe","Proliferate_DR"]
DR_or_not = Class_series.value_counts().tolist()
values = [DR_or_not[0], DR_or_not[1], DR_or_not[2], DR_or_not[3], DR_or_not[4]]
fig = px.pie(values=Class_series.value_counts(), names=lis , width=800, height=400, color_discrete_sequence=["skyblue","black","pink","purple","blue"]
             ,title="percentage among the different Severities of DR")
fig.show()

import pandas as pd
Class_series = pd.Series(y)
print("Number of images per class:")
for i, class_name in enumerate(lis):
    count = Class_series.value_counts().get(i, 0)
    print(f"{class_name}: {count}")

X[0].shape

X = X.reshape(X.shape[0], 128, 128, 3)

X[0].shape

gc.collect()

"""# **Data Split**"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=32, stratify=y)
#used stratify to balance the number of examples for each class

X_train.shape

X_test.shape

X_train.size

X_test.size

# Reshape the arrays
X_train = X_train.reshape((-1, 128, 128, 3))
X_test = X_test.reshape((-1, 128, 128, 3))



"""# **DenseNet121 Model Training**"""

from tensorflow.keras.applications.densenet import DenseNet121

model = DenseNet121(input_shape=(128, 128, 3),weights='imagenet',include_top=False)

model.trainable = True

set_trainable = False

for layer in model.layers:
    if layer.name == 'conv5_block16_0_bn':
        set_trainable = True
    if set_trainable:
        layer.trainable = True
    else:
        layer.trainable = False

for layer in model.layers[:]:
    if ('bn' in layer.name):
        trainable = False

from tensorflow.keras.layers import Dense,Flatten,Input,Dropout,Activation,BatchNormalization,Lambda

x = Flatten()(model.output)
x = tf.keras.layers.Dense(128,activation='relu')(x)
x = tf.keras.layers.Dropout(0.4)(x)
prediction = Dense(5,activation='softmax')(x)

from tensorflow.keras.models import Model

model = Model(inputs=model.input, outputs=prediction)

model.summary()

gc.collect()

from keras.callbacks import ModelCheckpoint,EarlyStopping

#Early Stopping

es = EarlyStopping(monitor='val_accuracy', min_delta = 0.005, patience=25, verbose=1, mode='auto')

#Model Check Point

mc = ModelCheckpoint(monitor='val_accuracy', filepath = 'densenet121_model.h5', verbose=1, save_best_only = True, mode='auto')

cd = [es,mc]

adam = keras.optimizers.Adam(learning_rate=0.0001)
model.compile(loss='sparse_categorical_crossentropy',
              optimizer = adam,
              metrics=['accuracy']
)

gc.collect()

history = model.fit(x=X_train, y=y_train,
          validation_data=(X_test, y_test),
          epochs=60,
          callbacks=cd,
          batch_size=64,
          shuffle=True)


print(history.history.keys())

train_loss = history.history['loss']
val_loss = history.history['val_loss']
train_acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

import matplotlib.pyplot as plt
# Plot training and validation accuracy
plt.figure()
plt.plot(train_acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim(min(train_acc + val_acc), max(train_acc + val_acc)) # Set y-axis limits
plt.legend()
plt.show()

# Plot training and validation loss
plt.figure()
plt.plot(train_loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

import matplotlib.pyplot as plt
import numpy as np
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

# Plotting Accuracy and Loss curves
plt.figure(figsize=(12, 6))

# Accuracy curve
plt.subplot(1, 2, 1)
plt.plot(train_acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.ylim([0, 1])  # Set y-axis limits
plt.legend()

# Loss curve
plt.subplot(1, 2, 2)
plt.plot(train_loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.ylim([0, 1])  # Set y-axis limits
plt.legend()

plt.tight_layout()
plt.show()

# Evaluate the model on the test data
loss, accuracy = model.evaluate(X_test, y_test, verbose=0)

print(f'Test Loss: {loss:.4f}')
print(f'Test Accuracy: {accuracy:.4f}')

# Make predictions
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)

# Print classification report
print("\nClassification Report:")
print(classification_report(y_test, y_pred_classes, target_names=lis))

import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

# Calculate the confusion matrix
cm = confusion_matrix(y_test, y_pred_classes)

# Print the confusion matrix using seaborn for better visualization
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=lis, yticklabels=lis)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc
from sklearn.preprocessing import label_binarize

# Binarize the true labels for ROC curve plotting (for multiclass)
y_test_bin = label_binarize(y_test, classes=[0, 1, 2, 3, 4])
n_classes = y_test_bin.shape[1]

# Compute ROC curve and ROC area for each class
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_pred[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Plot ROC curves for each class
plt.figure(figsize=(10, 8))
colors = ['aqua', 'darkorange', 'cornflowerblue', 'red', 'green']
for i, color in zip(range(n_classes), colors):
    plt.plot(fpr[i], tpr[i], color=color, lw=2,
             label='ROC curve of class {0} (area = {1:0.2f})'.format(lis[i], roc_auc[i]))

plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve for Multiclass')
plt.legend(loc="lower right")
plt.show()

import matplotlib.pyplot as plt
import numpy as np
# Select a few examples from the test set
num_examples = 5
sample_indices = np.random.choice(len(X_test), num_examples, replace=False)

# Get the sample images, true labels, and predicted labels
sample_images = X_test[sample_indices]
sample_true_labels = y_test[sample_indices]
sample_predicted_labels = y_pred_classes[sample_indices]

# Map class indices to class names
class_names = list(Classes.keys())

# Display the images with actual and predicted labels
plt.figure(figsize=(15, 5))
for i in range(num_examples):
    plt.subplot(1, num_examples, i + 1)
    # Ensure the image is in the correct format for plotting (remove the normalization)
    img_display = (sample_images[i] * 255).astype(np.uint8)
    plt.imshow(img_display)
    true_label_name = class_names[sample_true_labels[i]]
    predicted_label_name = class_names[sample_predicted_labels[i]]
    plt.title(f'Actual: {true_label_name}\nPredicted: {predicted_label_name}', fontsize=10)
    plt.axis('off')

plt.tight_layout()
plt.show()

!pip install --upgrade tensorflow-addons

import matplotlib.pyplot as plt
import numpy as np
from tensorflow.keras import backend as K

def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):
    # First, we create a model that maps the input image to the activations
    # of the last convolutional layer as well as the output predictions
    grad_model = tf.keras.models.Model(
        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]
    )

    # Then, we compute the gradient of the top predicted class for our input image
    # with respect to the activations of the last convolutional layer
    with tf.GradientTape() as tape:
        last_conv_layer_output, predictions = grad_model(img_array)
        if pred_index is None:
            pred_index = tf.argmax(predictions[0])
        class_channel = predictions[:, pred_index]

    # This is the gradient of the output neuron (top predicted or chosen)
    # with regard to the output feature map of the last convolutional layer
    grads = tape.gradient(class_channel, last_conv_layer_output)

    # This is a vector where each entry is the mean intensity of the gradient
    # over a specific feature map channel
    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))

    # We multiply each channel in the feature map array
    # by "how important" this channel is with regard to the top predicted class
    # then sum all the channels to obtain the heatmap class activation
    last_conv_layer_output = last_conv_layer_output[0]
    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]
    heatmap = tf.squeeze(heatmap)

    # Normalize the heatmap
    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)
    return heatmap.numpy()

def display_gradcam(img_path, heatmap, alpha=0.4):
    # Load the original image
    img = cv2.imread(img_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    # Resize heatmap to the original image size
    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))

    # Apply colormap to the heatmap
    heatmap = np.uint8(255 * heatmap)
    colormap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)

    # Superimpose the heatmap on the original image
    superimposed_img = colormap * alpha + img
    superimposed_img = np.clip(superimposed_img, 0, 255).astype(np.uint8)

    return superimposed_img

# Choose an image to visualize (replace with a path from your test set)
# For example, pick one from the sample images displayed earlier
sample_image_index_to_visualize = sample_indices[0]
image_to_visualize = X_test[sample_image_index_to_visualize]
true_label_visualize = y_test[sample_image_index_to_visualize]
predicted_label_visualize = y_pred_classes[sample_image_index_to_visualize]

# Reshape the image for model prediction (add batch dimension)
img_array_for_gradcam = np.expand_dims(image_to_visualize, axis=0)

# Get the predicted class index
pred_index = np.argmax(model.predict(img_array_for_gradcam))

# Find the name of the last convolutional layer in your model
# You can inspect model.summary() to find this.
# For DenseNet121, a good candidate might be within the last dense block.
# Let's try 'conv5_block16_concat' or 'conv5_block16_2_conv' if available
# If the specific layer name changes, you'll need to adjust this.
last_conv_layer_name = None
for layer in reversed(model.layers):
    if isinstance(layer, tf.keras.layers.Conv2D):
        last_conv_layer_name = layer.name
        break
print(f"Using last convolutional layer: {last_conv_layer_name}")


# Generate heatmap
heatmap = make_gradcam_heatmap(img_array_for_gradcam, model, last_conv_layer_name, pred_index=pred_index)

# Display the heatmap
plt.figure(figsize=(10, 5))

# Original Image
plt.subplot(1, 2, 1)
plt.imshow((image_to_visualize * 255).astype(np.uint8))
plt.title(f'Original\nActual: {class_names[true_label_visualize]}\nPredicted: {class_names[predicted_label_visualize]}', fontsize=10)
plt.axis('off')

# Heatmap
plt.subplot(1, 2, 2)
plt.imshow(image_to_visualize, cmap='gray') # Show original image as background (optional)
plt.imshow(heatmap, cmap='jet', alpha=0.4) # Superimpose heatmap
plt.title('Grad-CAM Heatmap', fontsize=10)
plt.axis('off')

plt.tight_layout()
plt.show()

# Note: Directly superimposing on the scaled image might be misleading.
# To truly superimpose on the original data scale, you would need the original unnormalized image.
# If you saved the original image paths, you could use those.
# The `display_gradcam` function attempts to resize the heatmap to an image,
# but without the original image file path, we'll skip calling it directly here
# and rely on plotting the heatmap over the unnormalized numpy array.

"""# **Training with attention based cnn model**"""

import matplotlib.pyplot as plt
import numpy as np
from tensorflow.keras.layers import GlobalAveragePooling2D, multiply, Permute, Reshape

def build_attention_cnn_model(input_shape=(128, 128, 3), num_classes=5):
    inputs = Input(shape=input_shape)

    # --- CNN Base ---
    x = Conv2D(32, (3, 3), padding='same', activation='relu')(inputs)
    x = BatchNormalization()(x)
    x = MaxPooling2D((2, 2))(x)

    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D((2, 2))(x)

    x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)
    x = BatchNormalization()(x)
    conv_output = MaxPooling2D((2, 2))(x) # Output before attention

    # --- Attention Mechanism (Channel Attention) ---
    # Squeeze and Excitation style attention
    attention = GlobalAveragePooling2D()(conv_output)
    attention = Dense(128 // 8, activation='relu', use_bias=False)(attention) # Reduced number of neurons
    attention = Dense(128, activation='sigmoid', use_bias=False)(attention) # Match number of channels
    attention = Reshape((1, 1, 128))(attention) # Reshape for channel-wise multiplication
    attention_output = multiply([conv_output, attention]) # Apply attention to the feature maps

    # --- Classification Head ---
    x = Flatten()(attention_output)
    x = Dense(128, activation='relu')(x)
    x = Dropout(0.4)(x)
    outputs = Dense(num_classes, activation='softmax')(x)

    model = Model(inputs=inputs, outputs=outputs)
    return model

# Build the Attention CNN model
attention_cnn_model = build_attention_cnn_model(input_shape=(128, 128, 3), num_classes=len(Classes))
attention_cnn_model.summary()

# Compile the model
attention_cnn_model.compile(loss='sparse_categorical_crossentropy',
                          optimizer=keras.optimizers.Adam(learning_rate=0.0001), # Adjust learning rate
                          metrics=['accuracy'])

# Early Stopping and Model Checkpoint
es_attn = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0.005, patience=10, verbose=1, mode='auto')
mc_attn = tf.keras.callbacks.ModelCheckpoint(filepath='attention_cnn_model.h5', monitor='val_accuracy', verbose=1, save_best_only=True, mode='auto')
cd_attn = [es_attn, mc_attn]

# Train the model
gc.collect()
history_attn = attention_cnn_model.fit(x=X_train, y=y_train,
                                   validation_data=(X_test, y_test),
                                   epochs=50, # You might need to adjust epochs
                                   callbacks=cd_attn,
                                   batch_size=64,
                                   shuffle=True)

# Evaluate the model
loss_attn, accuracy_attn = attention_cnn_model.evaluate(X_test, y_test, verbose=0)

print(f'Test Loss (Attention CNN): {loss_attn:.4f}')
print(f'Test Accuracy (Attention CNN): {accuracy_attn:.4f}')

# Make predictions
y_pred_attn = attention_cnn_model.predict(X_test)
y_pred_classes_attn = np.argmax(y_pred_attn, axis=1)

# Print classification report
print("\nClassification Report (Attention CNN):")
print(classification_report(y_test, y_pred_classes_attn, target_names=lis))

# Confusion Matrix
from sklearn.metrics import confusion_matrix
cm_attn = confusion_matrix(y_test, y_pred_classes_attn)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_attn, annot=True, fmt='d', cmap='Blues', xticklabels=lis, yticklabels=lis)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix (Attention CNN)')
plt.show()

# ROC Curve (Multiclass)
from sklearn.preprocessing import label_binarize
from sklearn.metrics import roc_curve, auc

y_test_bin_attn = label_binarize(y_test, classes=[0, 1, 2, 3, 4])
n_classes = y_test_bin_attn.shape[1]

fpr_attn = dict()
tpr_attn = dict()
roc_auc_attn = dict()
for i in range(n_classes):
    fpr_attn[i], tpr_attn[i], _ = roc_curve(y_test_bin_attn[:, i], y_pred_attn[:, i])
    roc_auc_attn[i] = auc(fpr_attn[i], tpr_attn[i])

plt.figure(figsize=(10, 8))
colors = ['aqua', 'darkorange', 'cornflowerblue', 'red', 'green']
for i, color in zip(range(n_classes), colors):
    plt.plot(fpr_attn[i], tpr_attn[i], color=color, lw=2,
             label='ROC curve of class {0} (area = {1:0.2f})'.format(lis[i], roc_auc_attn[i]))

plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve (Attention CNN)')
plt.legend(loc="lower right")
plt.show()

# Visualize predictions
num_examples = 5
sample_indices_attn = np.random.choice(len(X_test), num_examples, replace=False)

sample_images_attn = X_test[sample_indices_attn]
sample_true_labels_attn = y_test[sample_indices_attn]
sample_predicted_labels_attn = y_pred_classes_attn[sample_indices_attn]

class_names = list(Classes.keys())

plt.figure(figsize=(15, 5))
for i in range(num_examples):
    plt.subplot(1, num_examples, i + 1)
    img_display = (sample_images_attn[i] * 255).astype(np.uint8)
    plt.imshow(img_display)
    true_label_name = class_names[sample_true_labels_attn[i]]
    predicted_label_name = class_names[sample_predicted_labels_attn[i]]
    plt.title(f'Actual: {true_label_name}\nPredicted: {predicted_label_name}', fontsize=10)
    plt.axis('off')

plt.tight_layout()
plt.show()

# Grad-CAM for Attention CNN (using the last conv layer before attention)
# The last conv layer in the attention CNN is named 'conv2d_2' (or similar depending on build order)
last_conv_layer_name_attn = None
for layer in reversed(attention_cnn_model.layers):
    if isinstance(layer, tf.keras.layers.Conv2D):
        last_conv_layer_name_attn = layer.name
        break
print(f"Using last convolutional layer for Grad-CAM (Attention CNN): {last_conv_layer_name_attn}")

if last_conv_layer_name_attn:
    # Choose an image to visualize (e.g., the first sample from the visualization)
    sample_image_index_to_visualize_attn = sample_indices_attn[0]
    image_to_visualize_attn = X_test[sample_image_index_to_visualize_attn]
    true_label_visualize_attn = y_test[sample_image_index_to_visualize_attn]
    predicted_label_visualize_attn = y_pred_classes_attn[sample_image_index_to_visualize_attn]

    img_array_for_gradcam_attn = np.expand_dims(image_to_visualize_attn, axis=0)
    pred_index_attn = np.argmax(attention_cnn_model.predict(img_array_for_gradcam_attn))

    heatmap_attn = make_gradcam_heatmap(img_array_for_gradcam_attn, attention_cnn_model, last_conv_layer_name_attn, pred_index=pred_index_attn)
    superimposed_image_attn = superimpose_heatmap_on_image(image_to_visualize_attn, heatmap_attn)

    plt.figure(figsize=(12, 6))
    plt.subplot(1, 2, 1)
    display_img_attn = (image_to_visualize_attn * 255).astype(np.uint8) if np.max(image_to_visualize_attn) <= 1.01 else image_to_visualize_attn.astype(np.uint8)
    plt.imshow(display_img_attn)
    plt.title(f'Original\nActual: {class_names[true_label_visualize_attn]}\nPredicted: {class_names[predicted_label_visualize_attn]}', fontsize=10)
    plt.axis('off')

    plt.subplot(1, 2, 2)
    plt.imshow(superimposed_image_attn)
    plt.title('Grad-CAM Superimposed (Attention CNN)', fontsize=10)
    plt.axis('off')
    plt.tight_layout()
    plt.show()

    # Visualize Grad-CAM for one example from each class for the Attention CNN
    if not class_indices_to_visualize:
        print("Could not find any images in the test set for visualization.")
    else:
        plt.figure(figsize=(20, 10))

        for i, sample_image_index_to_visualize_attn in enumerate(class_indices_to_visualize): # Re-using the same class indices as before
            image_to_visualize_attn = X_test[sample_image_index_to_visualize_attn]
            true_label_visualize_attn = y_test[sample_image_index_to_visualize_attn]
            # Predict using the Attention CNN model
            img_array_for_gradcam_attn = np.expand_dims(image_to_visualize_attn, axis=0)
            y_pred_single_attn = attention_cnn_model.predict(img_array_for_gradcam_attn)
            predicted_label_visualize_attn = np.argmax(y_pred_single_attn)
            pred_index_attn = predicted_label_visualize_attn # Use the predicted index


            # Find the name of the last convolutional layer before attention
            last_conv_layer_name_attn = None
            for layer in reversed(attention_cnn_model.layers):
                 if isinstance(layer, tf.keras.layers.Conv2D):
                     last_conv_layer_name_attn = layer.name
                     break

            if last_conv_layer_name_attn is None:
                print("Could not find a convolutional layer in the Attention CNN model.")
                continue

            # Generate heatmap
            heatmap_attn = make_gradcam_heatmap(img_array_for_gradcam_attn, attention_cnn_model, last_conv_layer_name_attn, pred_index=pred_index_attn)

            # Superimpose heatmap on the original image array
            superimposed_image_attn = superimpose_heatmap_on_image(image_to_visualize_attn, heatmap_attn)

            # Display the superimposed image
            plt.subplot(1, len(class_indices_to_visualize), i + 1)
            plt.imshow(superimposed_image_attn)
            plt.title(f'{class_names[true_label_visualize_attn]}\n(Pred: {class_names[predicted_label_visualize_attn]})', fontsize=12)
            plt.axis('off')

        plt.suptitle("Grad-CAM Heatmaps (Attention CNN) showing important regions for prediction", fontsize=16)
        plt.tight_layout()
        plt.show()

else:
    print("Skipping Grad-CAM visualization for Attention CNN as the last convolutional layer name could not be found.")

from sklearn.metrics import confusion_matrix
cm_attn = confusion_matrix(y_test, y_pred_classes_attn)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_attn, annot=True, fmt='d', cmap='Blues', xticklabels=lis, yticklabels=lis)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix for DRNet')
plt.show()

from sklearn.preprocessing import label_binarize
from sklearn.metrics import roc_curve, auc

y_test_bin_attn = label_binarize(y_test, classes=[0, 1, 2, 3, 4])
n_classes = y_test_bin_attn.shape[1]

fpr_attn = dict()
tpr_attn = dict()
roc_auc_attn = dict()
for i in range(n_classes):
    fpr_attn[i], tpr_attn[i], _ = roc_curve(y_test_bin_attn[:, i], y_pred_attn[:, i])
    roc_auc_attn[i] = auc(fpr_attn[i], tpr_attn[i])

plt.figure(figsize=(10, 8))
colors = ['aqua', 'darkorange', 'cornflowerblue', 'red', 'green']
for i, color in zip(range(n_classes), colors):
    plt.plot(fpr_attn[i], tpr_attn[i], color=color, lw=2,
             label='ROC curve of class {0} (area = {1:0.2f})'.format(lis[i], roc_auc_attn[i]))

plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve for DRNet')
plt.legend(loc="lower right")
plt.show()

num_examples = 5
sample_indices_attn = np.random.choice(len(X_test), num_examples, replace=False)

sample_images_attn = X_test[sample_indices_attn]
sample_true_labels_attn = y_test[sample_indices_attn]
sample_predicted_labels_attn = y_pred_classes_attn[sample_indices_attn]

class_names = list(Classes.keys())

plt.figure(figsize=(15, 5))
for i in range(num_examples):
    plt.subplot(1, num_examples, i + 1)
    img_display = (sample_images_attn[i] * 255).astype(np.uint8)
    plt.imshow(img_display)
    true_label_name = class_names[sample_true_labels_attn[i]]
    predicted_label_name = class_names[sample_predicted_labels_attn[i]]
    plt.title(f'Actual: {true_label_name}\nPredicted: {predicted_label_name}', fontsize=10)
    plt.axis('off')

plt.tight_layout()
plt.show()

print("\n--- Final Evaluation Metrics ---")

# Training Metrics (from the last epoch)
print(f"Training Loss: {history.history['loss'][-1]:.4f}")
print(f"Training Accuracy: {history.history['accuracy'][-1]:.4f}")

# Test Metrics (from model.evaluate)
print(f'Test Loss: {loss:.4f}')
print(f'Test Accuracy: {accuracy:.4f}')

# Precision, Recall, F1-score (from the classification_report printed previously)
# To reiterate, we can explicitly calculate them or just confirm the output above.
# Let's just confirm the output from the classification_report.
print("\nClassification Report (Precision, Recall, F1-score):")
print(classification_report(y_test, y_pred_classes, target_names=lis))

# For the Attention CNN model as well:
print("\n--- Final Evaluation Metrics (Attention CNN) ---")

# Training Metrics (from the last epoch)
if 'loss' in history_attn.history and 'accuracy' in history_attn.history:
    print(f"Training Loss (Attention CNN): {history_attn.history['loss'][-1]:.4f}")
    print(f"Training Accuracy (Attention CNN): {history_attn.history['accuracy'][-1]:.4f}")
else:
    print("Training history for Attention CNN is incomplete.")


# Test Metrics (from model.evaluate)
print(f'Test Loss (Attention CNN): {loss_attn:.4f}')
print(f'Test Accuracy (Attention CNN): {accuracy_attn:.4f}')

# Precision, Recall, F1-score (from the classification_report for Attention CNN)
print("\nClassification Report (Attention CNN - Precision, Recall, F1-score):")
print(classification_report(y_test, y_pred_classes_attn, target_names=lis))

from sklearn.metrics import cohen_kappa_score, matthews_corrcoef, jaccard_score, confusion_matrix, roc_curve, auc
from sklearn.preprocessing import label_binarize
from tensorflow.keras.layers import GlobalAveragePooling2D, Reshape, multiply
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns


# Calculate Kappa score
kappa = cohen_kappa_score(y_test, y_pred_classes)
print(f"Kappa Score: {kappa:.4f}")

# Calculate MCC score
mcc = matthews_corrcoef(y_test, y_pred_classes)
print(f"MCC Score: {mcc:.4f}")

# Calculate Jaccard score (average over classes)
# The jaccard_score function can compute this for multiclass
jaccard = jaccard_score(y_test, y_pred_classes, average='macro') # 'macro' calculates score for each class and finds unweighted mean
print(f"Jaccard Score (macro avg): {jaccard:.4f}")

# You can also get the Jaccard score for each class if needed:
# jaccard_per_class = jaccard_score(y_test, y_pred_classes, average=None)
# print(f"Jaccard Score per class: {jaccard_per_class}")

# For the Attention CNN model:
print("\n--- Attention CNN Metrics ---")

# Calculate Kappa score for Attention CNN
kappa_attn = cohen_kappa_score(y_test, y_pred_classes_attn)
print(f"Kappa Score (Attention CNN): {kappa_attn:.4f}")

# Calculate MCC score for Attention CNN
mcc_attn = matthews_corrcoef(y_test, y_pred_classes_attn)
print(f"MCC Score (Attention CNN): {mcc_attn:.4f}")

# Calculate Jaccard score for Attention CNN
jaccard_attn = jaccard_score(y_test, y_pred_classes_attn, average='macro')
print(f"Jaccard Score (Attention CNN - macro avg): {jaccard_attn:.4f}")

# Optionally, you can print the classification reports again which already include precision, recall, f1-score
print("\nClassification Report (DenseNet Model):")
print(classification_report(y_test, y_pred_classes, target_names=lis))

print("\nClassification Report (Attention CNN Model):")
print(classification_report(y_test, y_pred_classes_attn, target_names=lis))

import matplotlib.pyplot as plt
# Plotting Accuracy and Loss curves for the Attention CNN model
plt.figure(figsize=(12, 6))

# Accuracy curve for Attention CNN
plt.subplot(1, 2, 1)
plt.plot(history_attn.history['accuracy'], label='Training Accuracy (Attention CNN)')
plt.plot(history_attn.history['val_accuracy'], label='Validation Accuracy (Attention CNN)')
plt.title('Attention CNN Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.ylim([0, 1])  # Set y-axis limits
plt.legend()

# Loss curve for Attention CNN
plt.subplot(1, 2, 2)
plt.plot(history_attn.history['loss'], label='Training Loss (Attention CNN)')
plt.plot(history_attn.history['val_loss'], label='Validation Loss (Attention CNN)')
plt.title('Attention CNN Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.ylim([0, 1])  # Set y-axis limits
plt.legend()

plt.tight_layout()
plt.show()

"""# **From this, these codes are for Future study**"""

import matplotlib.pyplot as plt
import numpy as np
from tensorflow.keras.layers import GlobalAveragePooling2D, Reshape, multiply, add, Dropout, Conv2D, MaxPooling2D, BatchNormalization, Input, Dense, Flatten
from sklearn.preprocessing import label_binarize
from sklearn.metrics import confusion_matrix, roc_curve, auc
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from tensorflow.keras.regularizers import l2
from tensorflow.keras.applications.densenet import preprocess_input
import cv2 # Import cv2 for image manipulation in Grad-CAM

# Define Grad-CAM helper functions within this cell
def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):
    # First, we create a model that maps the input image to the activations
    # of the last convolutional layer as well as the output predictions
    grad_model = tf.keras.models.Model(
        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]
    )

    # Then, we compute the gradient of the top predicted class for our input image
    # with respect to the activations of the last convolutional layer
    with tf.GradientTape() as tape:
        last_conv_layer_output, predictions = grad_model(img_array)
        if pred_index is None:
            pred_index = tf.argmax(predictions[0])
        class_channel = predictions[:, pred_index]

    # This is the gradient of the output neuron (top predicted or chosen)
    # with regard to the output feature map of the last convolutional layer
    grads = tape.gradient(class_channel, last_conv_layer_output)

    # This is a vector where each entry is the mean intensity of the gradient
    # over a specific feature map channel
    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))

    # We multiply each channel in the feature map array
    # by "how important" this channel is with regard to the top predicted class
    # then sum all the channels to obtain the heatmap class activation
    last_conv_layer_output = last_conv_layer_output[0]
    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]
    heatmap = tf.squeeze(heatmap)

    # Normalize the heatmap
    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)
    return heatmap.numpy()

# Function to superimpose heatmap on an image array
def superimpose_heatmap_on_image(img_array, heatmap, alpha=0.4):
    """Superimposes the heatmap on the image array."""
    # Resize heatmap to the image size
    heatmap = cv2.resize(heatmap, (img_array.shape[1], img_array.shape[0]))

    # Apply colormap to the heatmap
    heatmap = np.uint8(255 * heatmap)
    colormap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)

    # Convert image array to uint8 (assuming it's normalized 0-1 or float 0-255)
    # If your img_array is 0-1 float, scale it first
    if np.max(img_array) <= 1.01: # Simple check for normalization
        img_array = (img_array * 255).astype(np.uint8)
    else:
        img_array = img_array.astype(np.uint8)

    # Ensure image and heatmap have same depth (convert grayscale heatmap to 3 channels)
    if len(colormap.shape) == 2 and len(img_array.shape) == 3:
        colormap = cv2.cvtColor(colormap, cv2.COLOR_GRAY2BGR)
    elif len(colormap.shape) == 3 and len(img_array.shape) == 2:
         img_array = cv2.cvtColor(img_array, cv2.COLOR_GRAY2BGR)


    # Superimpose the heatmap on the original image
    superimposed_img = cv2.addWeighted(img_array, 1 - alpha, colormap, alpha, 0)
    superimposed_img = np.clip(superimposed_img, 0, 255).astype(np.uint8)

    return superimposed_img


def build_attention_cnn_model_v4(input_shape=(128, 128, 3), num_classes=5):
    inputs = Input(shape=input_shape)

    # --- CNN Base ---
    # Adjusted filter sizes and added more layers
    x = Conv2D(64, (7, 7), padding='same', activation='relu', kernel_regularizer=l2(0.001))(inputs) # Increased L2 regularization
    x = BatchNormalization()(x)
    x = MaxPooling2D((2, 2))(x)
    x = Dropout(0.3)(x) # Increased Dropout

    x = Conv2D(128, (5, 5), padding='same', activation='relu', kernel_regularizer=l2(0.001))(x) # Increased L2 regularization
    x = BatchNormalization()(x)
    x = MaxPooling2D((2, 2))(x)
    x = Dropout(0.3)(x) # Increased Dropout

    x = Conv2D(256, (3, 3), padding='same', activation='relu', kernel_regularizer=l2(0.001))(x) # Increased L2 regularization
    x = BatchNormalization()(x)
    conv_output = MaxPooling2D((2, 2))(x) # Output before attention
    x = Dropout(0.3)(x) # Increased Dropout


    # --- Attention Mechanism (Combined Channel and Spatial Attention) ---
    # Channel Attention
    channel_attention = GlobalAveragePooling2D()(conv_output)
    channel_attention = Dense(conv_output.shape[-1] // 16, activation='relu', use_bias=False)(channel_attention)
    channel_attention = Dense(conv_output.shape[-1], activation='sigmoid', use_bias=False)(channel_attention)
    channel_attention = Reshape((1, 1, conv_output.shape[-1]))(channel_attention)
    attended_features = multiply([conv_output, channel_attention])

    # Spatial Attention
    spatial_attention = Conv2D(1, (7, 7), padding='same', activation='sigmoid')(attended_features) # Larger kernel
    attended_features = multiply([attended_features, spatial_attention])

    # Optional: Add the original features back (residual connection)
    # This can help the model learn to use the original features as well
    # attended_features = add([conv_output, attended_features])


    # --- Classification Head ---
    x = Flatten()(attended_features)
    x = Dense(256, activation='relu', kernel_regularizer=l2(0.001))(x) # Increased L2 regularization
    x = Dropout(0.6)(x) # Increased Dropout rate
    outputs = Dense(num_classes, activation='softmax')(x)

    model = Model(inputs=inputs, outputs=outputs)
    return model

# Build the improved Attention CNN model
attention_cnn_model_v4 = build_attention_cnn_model_v4(input_shape=(128, 128, 3), num_classes=len(Classes))
attention_cnn_model_v4.summary()

# Compile the model
# Adjusted learning rate and added regularization
adam_v4 = keras.optimizers.Adam(learning_rate=0.00005) # Slightly reduced learning rate
attention_cnn_model_v4.compile(loss='sparse_categorical_crossentropy',
                          optimizer=adam_v4,
                          metrics=['accuracy'])

# Early Stopping and Model Checkpoint
es_attn_v4 = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0.005, patience=25, verbose=1, mode='auto', restore_best_weights=True) # Increased patience
mc_attn_v4 = tf.keras.callbacks.ModelCheckpoint(filepath='attention_cnn_model_v4.keras', monitor='val_accuracy', verbose=1, save_best_only=True, mode='auto') # Save in Keras native format
# Add ReduceLROnPlateau
reduce_lr_v4 = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=7, min_lr=0.000001, verbose=1) # Increased patience for LR reduction

cd_attn_v4 = [es_attn_v4, mc_attn_v4, reduce_lr_v4]

# Data Augmentation (to reduce overfitting)
data_augmentation = keras.Sequential([
  layers.RandomFlip("horizontal_and_vertical"),
  layers.RandomRotation(0.15), # Slightly increased rotation
  layers.RandomZoom(0.15), # Slightly increased zoom
  layers.RandomContrast(0.15), # Slightly increased contrast
  layers.RandomTranslation(height_factor=0.15, width_factor=0.15), # Slightly increased translation
])

# Train the model with data augmentation
print("Training the improved Attention CNN model (V4)...")
gc.collect()
history_attn_v4 = attention_cnn_model_v4.fit(
    data_augmentation(X_train), y_train, # Apply data augmentation
    validation_data=(X_test, y_test),
    epochs=150, # Increased epochs, relying on Early Stopping
    callbacks=cd_attn_v4,
    batch_size=32, # Smaller batch size can sometimes help regularization
    shuffle=True
)

print("\nEvaluating the improved Attention CNN model (V4)...")
# Evaluate the model
loss_attn_v4, accuracy_attn_v4 = attention_cnn_model_v4.evaluate(X_test, y_test, verbose=0)

print(f'Test Loss (Improved Attention CNN V4): {loss_attn_v4:.4f}')
print(f'Test Accuracy (Improved Attention CNN V4): {accuracy_attn_v4:.4f}')

# Make predictions
y_pred_attn_v4 = attention_cnn_model_v4.predict(X_test)
y_pred_classes_attn_v4 = np.argmax(y_pred_attn_v4, axis=1)

# Print classification report
print("\nClassification Report (Improved Attention CNN V4):")
print(classification_report(y_test, y_pred_classes_attn_v4, target_names=lis))

# Confusion Matrix
cm_attn_v4 = confusion_matrix(y_test, y_pred_classes_attn_v4)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_attn_v4, annot=True, fmt='d', cmap='Blues', xticklabels=lis, yticklabels=lis)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix (Improved Attention CNN V4)')
plt.show()

# ROC Curve (Multiclass)

y_test_bin_attn_v4 = label_binarize(y_test, classes=[0, 1, 2, 3, 4])
n_classes = y_test_bin_attn_v4.shape[1]

fpr_attn_v4 = dict()
tpr_attn_v4 = dict()
roc_auc_attn_v4 = dict()
for i in range(n_classes):
    fpr_attn_v4[i], tpr_attn_v4[i], _ = roc_curve(y_test_bin_attn_v4[:, i], y_pred_attn_v4[:, i])
    roc_auc_attn_v4[i] = auc(fpr_attn_v4[i], tpr_attn_v4[i])

plt.figure(figsize=(10, 8))
colors = ['aqua', 'darkorange', 'cornflowerblue', 'red', 'green']
for i, color in zip(range(n_classes), colors):
    plt.plot(fpr_attn_v4[i], tpr_attn_v4[i], color=color, lw=2,
             label='ROC curve of class {0} (area = {1:0.2f})'.format(lis[i], roc_auc_attn_v4[i]))

plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve (Improved Attention CNN V4)')
plt.legend(loc="lower right")
plt.show()


# Plotting Accuracy and Loss curves for the Improved Attention CNN model
plt.figure(figsize=(12, 6))

# Accuracy curve for Improved Attention CNN
plt.subplot(1, 2, 1)
plt.plot(history_attn_v4.history['accuracy'], label='Training Accuracy (Improved Attention CNN V4)')
plt.plot(history_attn_v4.history['val_accuracy'], label='Validation Accuracy (Improved Attention CNN V4)')
plt.title('Improved Attention CNN Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.ylim([0, 1])  # Set y-axis limits
plt.legend()

# Loss curve for Improved Attention CNN
plt.subplot(1, 2, 2)
plt.plot(history_attn_v4.history['loss'], label='Training Loss (Improved Attention CNN V4)')
plt.plot(history_attn_v4.history['val_loss'], label='Validation Loss (Improved Attention CNN V4)')
plt.title('Improved Attention CNN Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.ylim([0, 1])  # Set y-axis limits
plt.legend()

plt.tight_layout()
plt.show()

# Redefine class_indices_to_visualize for Grad-CAM visualization
class_indices_to_visualize = []
for class_idx in range(len(lis)): # Iterate through all classes
    # Find an index in X_test that belongs to this class
    indices_of_class = np.where(y_test == class_idx)[0]
    if len(indices_of_class) > 0:
        # Take the first index found for this class
        class_indices_to_visualize.append(indices_of_class[0])
    if len(class_indices_to_visualize) == len(lis): # Stop if we have one example for each class
        break

# Grad-CAM for Improved Attention CNN
# The last conv layer before the attention mechanisms
last_conv_layer_name_attn_v4 = None
for layer in reversed(attention_cnn_model_v4.layers):
    if isinstance(layer, tf.keras.layers.Conv2D) and 'spatial_attention' not in layer.name: # Exclude the spatial attention conv layer
        last_conv_layer_name_attn_v4 = layer.name
        break
print(f"Using last convolutional layer for Grad-CAM (Improved Attention CNN V4): {last_conv_layer_name_attn_v4}")

if last_conv_layer_name_attn_v4:
    # Visualize Grad-CAM for one example from each class for the Improved Attention CNN
    if not class_indices_to_visualize:
        print("Could not find any images in the test set for visualization.")
    else:
        plt.figure(figsize=(20, 10))

        for i, sample_image_index_to_visualize_attn_v4 in enumerate(class_indices_to_visualize): # Re-using the same class indices
            image_to_visualize_attn_v4 = X_test[sample_image_index_to_visualize_attn_v4]
            true_label_visualize_attn_v4 = y_test[sample_image_index_to_visualize_attn_v4]
            # Predict using the Improved Attention CNN model
            img_array_for_gradcam_attn_v4 = np.expand_dims(image_to_visualize_attn_v4, axis=0)
            y_pred_single_attn_v4 = attention_cnn_model_v4.predict(img_array_for_gradcam_attn_v4)
            predicted_label_visualize_attn_v4 = np.argmax(y_pred_single_attn_v4)
            pred_index_attn_v4 = predicted_label_visualize_attn_v4 # Use the predicted index

            # Generate heatmap
            heatmap_attn_v4 = make_gradcam_heatmap(img_array_for_gradcam_attn_v4, attention_cnn_model_v4, last_conv_layer_name_attn_v4, pred_index=pred_index_attn_v4)

            # Superimpose heatmap on the original image array
            superimposed_image_attn_v4 = superimpose_heatmap_on_image(image_to_visualize_attn_v4, heatmap_attn_v4)

            # Display the superimposed image
            plt.subplot(1, len(class_indices_to_visualize), i + 1)
            plt.imshow(superimposed_image_attn_v4)
            plt.title(f'{class_names[true_label_visualize_attn_v4]}\n(Pred: {class_names[predicted_label_visualize_attn_v4]})', fontsize=12)
            plt.axis('off')

        plt.suptitle("Grad-CAM Heatmaps (Improved Attention CNN V4) showing important regions for prediction", fontsize=16)
        plt.tight_layout()
        plt.show()

else:
    print("Skipping Grad-CAM visualization for Improved Attention CNN V4 as the last relevant convolutional layer name could not be found.")